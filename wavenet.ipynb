{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfffc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics.functional import auroc as AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8824b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "991eb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc02188",
   "metadata": {},
   "source": [
    "## 超参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9fe1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "NUM_CLASS =  13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13043eaf",
   "metadata": {},
   "source": [
    "## 数据集读取与归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a55f0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"0\": 0,\n",
    "             \"1\": 1,\n",
    "             \"2\": 2,\n",
    "             \"3\": 3,\n",
    "             \"4\": 4,\n",
    "             \"5\": 5,\n",
    "             \"6\": 6,\n",
    "             \"7\": 7,\n",
    "             \"8\": 8,\n",
    "             \"9\": 9,\n",
    "             \"10\": 10,\n",
    "             \"11\": 11,\n",
    "             \"12\": 9,\n",
    "             \"13\": 10,\n",
    "             \"14\": 11,\n",
    "             \"15\":12\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89be9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f93026e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_samples(files):\n",
    "    '''\n",
    "    输出[batch_size,seq_len.feature_dim]的数据\n",
    "    其中seq_len为平均序列长度，过长的截断，\n",
    "    过短的使用时间序列最后一个时刻数据重复\n",
    "    \n",
    "    '''\n",
    "    original_sample_features = []\n",
    "    original_sample_label = []\n",
    "    seq_len = [] \n",
    "    for filename in files:\n",
    "        with open(filename,\"rb\") as f:\n",
    "            num_label=filename.split(\".\")[0]\n",
    "            _,sample_num_str,label_str = num_label.split(\"_\")\n",
    "            original_lable =label_str\n",
    "            label=label_map[original_lable]\n",
    "            original_sample_label.append(label)\n",
    "            feature = pkl.load(f)\n",
    "            original_sample_features.append(feature)\n",
    "            seq_len.append(len(feature)) \n",
    "    mean_seq_len = int(np.mean(seq_len))\n",
    "    ##重新处理数据,长的截断，短的补0 \n",
    "    sample_features = []\n",
    "    for idx,item in enumerate(original_sample_features):\n",
    "        if item.shape[0] > mean_seq_len:\n",
    "            item = item[:mean_seq_len,:]\n",
    "        else:\n",
    "            last = item[-1,:].reshape(1,-1)\n",
    "            padding = np.ones((mean_seq_len-item.shape[0],item.shape[1]))*last\n",
    "            item = np.concatenate((item,padding))\n",
    "        sample_features.append(item)\n",
    "    sample_features = np.array(sample_features)\n",
    "    original_sample_label=np.array(original_sample_label)\n",
    "    return sample_features,original_sample_label,mean_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd92b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob(\"data/*.pkl\")\n",
    "sample_features,sample_label,mean_seq_len=get_all_samples(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95cfa0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_features),len(sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d71c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "##直接对所有数据集归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "w=sample_features.reshape((-1,sample_features.shape[2]))\n",
    "batch_size,seq_len,feature_dim = sample_features.shape\n",
    "new_feature=scaler.fit_transform(w)\n",
    "normaled_feature = new_feature.reshape((batch_size,seq_len,feature_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa286f9",
   "metadata": {},
   "source": [
    "## 切分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1c2fa",
   "metadata": {},
   "source": [
    " 取出30% 数据作为最终的test 集,确保测试集和训练集一样各类别的样本个数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dd26120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0bbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d0c0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=normaled_feature ,sample_label\n",
    "for train_index, test_index in sp.split(X,y):\n",
    "    X_train_vaild, X_test = X[train_index], X[test_index]\n",
    "    y_train_vaild, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54e846",
   "metadata": {},
   "source": [
    "使用StratifiedKFold划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72e7fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1dadbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=NUM_FOLDS,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10ab2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf= cv.split(X_train_vaild,y_train_vaild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7299385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_set = []\n",
    "val_index_set = []\n",
    "for fold, (train_index, val_index) in enumerate(kf):\n",
    "    train_index_set.append(train_index)\n",
    "    val_index_set.append(val_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc17164",
   "metadata": {},
   "source": [
    "# 转化为数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd78f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  MyDataset(Dataset):\n",
    "    def __init__(self,feature_data,target_data=None,train_lag=True):\n",
    "        super(MyDataset,self).__init__()\n",
    "        self.feature_data = feature_data\n",
    "        self.train_lag = train_lag \n",
    "        self.target_data = target_data \n",
    "    def __len__(self):\n",
    "        return len(self.feature_data)\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.feature_data[idx]\n",
    "        if self.train_lag:\n",
    "            y = self.target_data[idx]\n",
    "            return {\"feature\":torch.Tensor(x),\n",
    "                    \"label\":torch.LongTensor([y])}\n",
    "        else:\n",
    "            return {\"feature\":torch.Tensor(x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6e2495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_index_set,val_index_set,fold_idx,batch_size,scaler=None):\n",
    "        super().__init__()\n",
    "        self.train_index_set = train_index_set\n",
    "        self.val_index_set = val_index_set \n",
    "        self.fold_idx = fold_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = scaler \n",
    "    def prepare(self,feature_data,target_data):\n",
    "        self.total_dataset= self.get_input(feature_data,target_data,train_lag=True)\n",
    "        train_indices = self.train_index_set[self.fold_idx]\n",
    "        val_indices = self.val_index_set[self.fold_idx]\n",
    "        #self.train_ds = torch.utils.data.Subset(self.total_dataset,train_indices)\n",
    "        #self.val_ds = torch.utils.data.Subset(self.total_dataset,val_indices)\n",
    "        self.train_sampler = SubsetRandomSampler(train_indices)\n",
    "        self.valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.total_dataset, self.batch_size,sampler=self.train_sampler,shuffle=False)     \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.total_dataset, self.batch_size,sampler =self.valid_sampler,shuffle=False)  \n",
    "    @staticmethod\n",
    "    def get_input(feature_data,target_data,train_lag=True,test_lag = False):\n",
    "        ##额外的测试数据需要采用和训练/Valid一样的scaler归一化\n",
    "        if test_lag:\n",
    "            batch_size,seq_len,feature_dim = feature_data.shape\n",
    "            w=feature_data.reshape((-1,feature_dim))\n",
    "            new_feature=self.scaler.fit_transform(w)\n",
    "            feature_data = new_feature.reshape((batch_size,seq_len,feature_dim))\n",
    "        if train_lag:\n",
    "            pass\n",
    "        else:\n",
    "            target_data = None \n",
    "        my_dataset = MyDataset(feature_data,target_data,train_lag)\n",
    "        return my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6563b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed4eadb",
   "metadata": {},
   "source": [
    "'''测试 获取数据集'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf0ae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=MyDataModule(train_index_set,val_index_set,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5b7f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=MyDataModule(train_index_set,val_index_set,1,10)\n",
    "dm.prepare(X_train_vaild,y_train_vaild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4d00170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader =dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6eb61",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8224d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight=None, reduction='mean', gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "050ce226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Input and output sizes will be the same.\n",
    "    #[in_size+2*pad-dilation*(kernel_size-1)-1]/stride+1  \n",
    "    if stride ==1:\n",
    "    then \n",
    "        out_size = in_size+2*pad-dilation*(kernel_size-1)\n",
    "    finially:\n",
    "        out_size = in_size \n",
    "    recommand:\n",
    "        stride = 1\n",
    "        kernel_size % 2 =1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, out_size, kernel_size, dilation=1,stride=1):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.pad = (kernel_size-1) // 2 * dilation\n",
    "        self.conv1 = nn.Conv1d(in_size, out_size, kernel_size, padding=self.pad, stride=stride,dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9af6538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):    \n",
    "    def __init__(self, residual_size, skip_size, dilation):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv_filter = CausalConv1d(residual_size, residual_size,\n",
    "                                         kernel_size=3, dilation=dilation)\n",
    "        self.conv_gate = CausalConv1d(residual_size, residual_size,\n",
    "                                         kernel_size=3, dilation=dilation)   \n",
    "        self.resconv1_1 = nn.Conv1d(residual_size, residual_size, kernel_size=1)\n",
    "        self.skipconv1_1 = nn.Conv1d(residual_size, skip_size, kernel_size=1)\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        conv_filter = self.conv_filter(x)\n",
    "        conv_gate = self.conv_gate(x)  \n",
    "        fx = torch.tanh(conv_filter) * torch.sigmoid(conv_gate)\n",
    "        fx = self.resconv1_1(fx) \n",
    "        skip = self.skipconv1_1(fx) \n",
    "        residual = fx + x  \n",
    "        #residual=[batch,residual_size,seq_len]  skip=[batch,skip_size,seq_len]\n",
    "        return skip, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d34a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedStack(nn.Module):\n",
    "    def __init__(self, residual_size, skip_size, dilation_depth):\n",
    "        super(DilatedStack, self).__init__()\n",
    "        residual_stack = [ResidualLayer(residual_size, skip_size, 2**layer)\n",
    "                         for layer in range(dilation_depth)]\n",
    "        self.residual_stack = nn.ModuleList(residual_stack)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.residual_stack:\n",
    "            skip, x = layer(x)\n",
    "            skips.append(skip.unsqueeze(0))\n",
    "            #skip =[1,batch,skip_size,seq_len]\n",
    "        return torch.cat(skips, dim=0), x  # [layers,batch,skip_size,seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87a25b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args=args\n",
    "        self.save_hyperparameters()\n",
    "        self.input_size = args.input_size \n",
    "        #self.out_size = args.out_size\n",
    "        self.num_class = args.num_class \n",
    "        self.residual_size =args.residual_size \n",
    "        self.skip_size =args.skip_size \n",
    "        self.dilation_cycles = args.dilation_cycles \n",
    "        self.dilation_depth = args.dilation_depth \n",
    "        \n",
    "        self.input_conv = CausalConv1d(self.input_size,self.residual_size, kernel_size=3)        \n",
    "\n",
    "        self.dilated_stacks = nn.ModuleList(\n",
    "\n",
    "            [DilatedStack(self.residual_size, self.skip_size, self.dilation_depth)\n",
    "\n",
    "             for cycle in range(self.dilation_cycles)]\n",
    "\n",
    "        )\n",
    "\n",
    "        self.convout_1 = nn.Conv1d(self.skip_size, self.skip_size, kernel_size=1)\n",
    "        self.finial=nn.Linear(self.skip_size,self.num_class)\n",
    "        \n",
    "        #crition\n",
    "        self.criterion = FocalLoss(gamma=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.permute(0,2,1)# [batch,input_feature_dim, seq_len] =[N,C_in,Lin]\n",
    "     \n",
    "        x = self.input_conv(x) # [batch,residual_size, seq_len]             \n",
    "\n",
    "        skip_connections = []\n",
    "\n",
    "        for cycle in self.dilated_stacks:\n",
    "\n",
    "            skips, x = cycle(x)             \n",
    "            skip_connections.append(skips)\n",
    "\n",
    "        ## skip_connection=[total_layers,batch,skip_size,seq_len]\n",
    "        skip_connections = torch.cat(skip_connections, dim=0)        \n",
    "\n",
    "        # gather all output skip connections to generate output, discard last residual output\n",
    "\n",
    "        out = skip_connections.sum(dim=0) # [batch,skip_size,seq_len]\n",
    "\n",
    "        out = F.relu(out)\n",
    "        out = self.convout_1(out) # [batch,out_size,seq_len]\n",
    "        \n",
    "        out=torch.mean(out,dim=2)#=[batch,out_size]\n",
    "\n",
    "        out=self.finial(out) #[batch,out_size]=>[batch,num_class]\n",
    "        return out\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X,y=batch[\"feature\"],batch[\"label\"]\n",
    "        predict=self(X)\n",
    "        target = torch.squeeze(y,1)\n",
    "        loss=self.criterion(predict,target)\n",
    "        self.log(\"train_loss\",loss,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        X,y=batch[\"feature\"],batch[\"label\"]\n",
    "        predict=self(X)\n",
    "        target = torch.squeeze(y,1)\n",
    "        loss=self.criterion(predict,target)\n",
    "        self.log(\"val_loss\",loss,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        X=batch[\"feature\"]\n",
    "        predict=self(X)\n",
    "        return predict \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692a10",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a106e2",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "88142cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    num_epochs=10\n",
    "    batch_size=10\n",
    "\n",
    "    input_size = feature_dim\n",
    "    num_class = NUM_CLASS\n",
    "    ##通道数，相当于特征数\n",
    "    residual_size = 32\n",
    "    skip_size = 256\n",
    "    dilation_cycles = 3 \n",
    "    dilation_depth =   8  #2**8 = 256 \n",
    "                          #3*256 = 768 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10c78aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##original paper seuqnece_length = 160000  dilation =[1,2,4,...,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cdd1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75893f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    from pytorch_lightning.loggers import TensorBoardLogger \n",
    "    logger=TensorBoardLogger(\"tensorboard_logs\",name=f\"fold_{fold}\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filename=f\"fold_{fold}_best_loss\",\n",
    "        monitor=\"train_loss\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        save_last=False,\n",
    "    )\n",
    "    earystopping = EarlyStopping(monitor=\"val_loss\")\n",
    "    trainer = pl.Trainer(  \n",
    "    gpus=-1 if torch.cuda.is_available() else None, \n",
    "    #precision=16,\n",
    "    max_epochs=args.num_epochs,\n",
    "    callbacks = [checkpoint_callback,earystopping],\n",
    "    logger=logger\n",
    "    )\n",
    "    \n",
    "    dm=MyDataModule(train_index_set,val_index_set,fold,args.batch_size)\n",
    "    dm.prepare(X_train_vaild,y_train_vaild)\n",
    "    model = WaveNet(args)\n",
    "    \n",
    "    trainer.fit(model,dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5041bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name           | Type         | Params\n",
      "------------------------------------------------\n",
      "0 | input_conv     | CausalConv1d | 416   \n",
      "1 | dilated_stacks | ModuleList   | 377 K \n",
      "2 | convout_1      | Conv1d       | 65.8 K\n",
      "3 | finial         | Linear       | 3.3 K \n",
      "4 | criterion      | FocalLoss    | 0     \n",
      "------------------------------------------------\n",
      "446 K     Trainable params\n",
      "0         Non-trainable params\n",
      "446 K     Total params\n",
      "1.787     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/anaconda3/lib/python3.8/site-packages/pytorch_lightning-1.5.0rc1-py3.8.egg/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/deeplearning/anaconda3/lib/python3.8/site-packages/pytorch_lightning-1.5.0rc1-py3.8.egg/pytorch_lightning/trainer/data_loading.py:387: UserWarning: The number of training samples (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c12d64d0ad42a9b419219f8f021b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fold(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbc5a9",
   "metadata": {},
   "source": [
    "## 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90933e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(folds=5,path=\"tensorboard_logs/\"):\n",
    "    import os \n",
    "    models = []\n",
    "    for i in range(folds):\n",
    "        filename = os.path.join(path,f\"fold_{i}/version_0/checkpoints/fold_{i}_best_loss.ckpt\")\n",
    "        model = WaveNet.load_from_checkpoint(filename)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db8a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cc42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46531bef",
   "metadata": {},
   "source": [
    "## 模型效果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29f5a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score \n",
    "def get_score(y_true,y_pred):\n",
    "    y_true = np.array(y_true).reshape(-1,1)\n",
    "    y_pred = np.array(y_pred).reshape(-1,1)\n",
    "    f1 =f1_score(y_true,y_pred,average=\"macro\")*100\n",
    "    p =precision_score(y_true,y_pred,average=\"macro\")*100\n",
    "    r = recall_score(y_true,y_pred,average=\"macro\")*100\n",
    "    return str((reformat(p,2),reformat(r,2),reformat(f1,2))),reformat(f1,2)\n",
    "def reformat(num,n):\n",
    "    return float(format(num,\"0.\"+str(n)+\"f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f49561",
   "metadata": {},
   "source": [
    "把5折交叉验证中最好的模型作平均预测,针对X_test和y_test数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de875055",
   "metadata": {},
   "outputs": [],
   "source": [
    "##带有标签，因为所有数据一起做了归一化，无需再归一化\n",
    "test_dataset = MyDataModule.get_input( \n",
    "    X_test,\n",
    "    y_test,\n",
    "    train_lag=True,\n",
    "    test_lag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9983b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,batch_size= 10,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a844675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = torch.Size([batch_size,seq_len, feature_dim])\n",
    "#label = torch.Size([batch_size,1])\n",
    "#ouput = torch.Size([batch_size,num_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9219db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pro = []\n",
    "total_true_labels = [] \n",
    "device = 'cuda'\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    pro_list= []\n",
    "    for item in test_dataloader:\n",
    "        features = item[\"feature\"]\n",
    "        label = item[\"label\"]\n",
    "        logist = model(features)\n",
    "        pro = torch.softmax(logist,dim=1).detach().cpu().numpy()\n",
    "        pro_list.extend(pro)\n",
    "        \n",
    "    total_pro.append(pro_list)\n",
    "        #output=torch.softmax(logit,dim=1)\n",
    "        #pred_label = torch.argmax(output,dim=1).cpu().numpy().tolist()\n",
    "        #true_label = label.cpu().numpy().reshape(-1,)\n",
    "        #results.extend(pred_label)\n",
    "        #true_labels.extend(true_label)\n",
    "    #total_results.append(results)\n",
    "    #total_true_labels.append(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b22a1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pro = np.array(total_pro)\n",
    "avg_pro = np.mean(total_pro,axis=0)\n",
    "pred_labels = np.argmax(avg_pro,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53e1ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('(2.34, 9.5, 3.29)', 3.29)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(y_test,pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c35de4",
   "metadata": {},
   "source": [
    "## classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62d5c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c01cc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [str(item) for item in range(NUM_CLASS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c6bb4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(y_test))==NUM_CLASS:\n",
    "    print(classification_report(y_test,pred_labels,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeedab3",
   "metadata": {},
   "source": [
    "## ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d211c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df3b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4abf7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "import matplotlib.pyplot as plt \n",
    "def plot_confusion_matrix(cm,classes,normalize = False,title=\"Confusion Matrix\",\n",
    "                          cmap = plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\")/cm.sum(axis = 1)[:np.newaxis]\n",
    "        print(\"normalize confusion matrix\")\n",
    "    else:\n",
    "        print(\"confusion matrix without normalize\")\n",
    "        print(cm)\n",
    "    plt.imshow(cm,interpolation=\"nearest\",cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation = 45)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "        \n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh =cm.max()/2.0 \n",
    "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j,i,format(cm[i,j],fmt),\n",
    "                horizontalalignment = 'center',\n",
    "                color = 'white' if cm[i,j] > thresh else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974bfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
