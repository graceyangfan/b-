{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比赛整体的思路："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、看比赛的背景介绍，比赛的评分方式，主办方对所有字段含义的解释（如果是匿名赛就更复杂了，好在这次算是半匿名，只有部分特征是含义不清的），查看过去相似的比赛以及获奖选手的思路分享与开源代码， 很多时候别人发布过的代码可以直接快速的复用；\n",
    "2、eda 数据探索，通过可视化、计算统计值等寻找数据中的异常之处，并且在这个过程中进一步熟悉手头的数据，在这个过程中可以参考一些开源的kernel，但是尽量自己思考，而不是直接抄袭，参加比赛的主要目的是学习与自我提升，要建立起自己的思维框架来解决问题；\n",
    "3、性能优化，包括了原始数据的数据类型的优化以降低内存占用与存储、一些常用的函数的定义避免过于冗余的代码、一些运行时间很久的函数的性能优化以提高试验的速度与效率，文件快速的读取与存储、原始数据的采样等等；\n",
    "4、建立一个baseline，后续在这个baseline上进行深入的调整，如果实在时间着急，找一个比较恰当合适的baseline；\n",
    "5、确定一个好的cv策略（对于无序的问题，可能cv就是简单的kfold或者stratifiedkfold，但是对于时序相关的问题，cv的选择还非常较重要的，如何尽量降低时间穿越的影响是一件很重要的事）；\n",
    "6、根据模型的表现对症下药找到问题所在然后实施对应的特征工程方法衍生新的特征、转换原始特征或者进行特征选择等；\n",
    "7、在这个过程中不断的阅读新的kernel和discussion来寻找新的灵感\n",
    "8、使用stacking等集成方法与后处理技巧等方法来上分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 性能优化，提高建模效率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "大限度减少pandas的内存占用 by 使用更少内存占用的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "对于类别特征，还有更好的处理思路：类别就行lable encoder，用int型数据存放类别特征，大大减少内存占用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=None,test=None,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; \n",
    "    gc.collect()\n",
    "    if verbose: print(nm,', ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "或者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "使用numba提速 评价指标的计算，也可用于其它指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 相关性处理的正确姿势，通过将一些常用的功能编写进入函数实现快速的相关特征的删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def nan_search(data=None):\n",
    "    nans_df = data.isna()\n",
    "    nans_groups={}\n",
    "    for col in data.columns:\n",
    "        cur_group = nans_df[col].sum()\n",
    "        try:\n",
    "            nans_groups[cur_group].append(col)\n",
    "        except:\n",
    "            nans_groups[cur_group]=[col]\n",
    "    del nans_df; \n",
    "    gc.collect()\n",
    "    length=data.shape[0]\n",
    "    groups=[]\n",
    "    ratio=[]\n",
    "    for k,v in nans_groups.items():\n",
    "        \n",
    "        print('####### NAN count =',k)\n",
    "        print('####### NAN ratio =',k/length)\n",
    "        ratio.append(k/length)\n",
    "        print(v)\n",
    "        groups.append(v)\n",
    "        print('\\n')\n",
    "    return groups,ratio\n",
    "        \n",
    "def reduce_group(data,cols):\n",
    "    drops = []\n",
    "    for g in cols:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = data[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        g.remove(vx)\n",
    "        drops.append(g)\n",
    "    print('drop these',drops)\n",
    "    return drops\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def identify_collinear(corr_matrix,correlation_threshold,X):\n",
    "    \n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "    \n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "    \n",
    "    # Dataframe to hold correlated pairs\n",
    "    record_collinear = pd.DataFrame(columns = ['corr_feature1', 'corr_feature2', 'corr_value'])\n",
    "    \n",
    "    # Iterate through the columns to drop to record pairs of correlated features\n",
    "    for column in to_drop:\n",
    "    \n",
    "        # Find the correlated features\n",
    "        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "    \n",
    "        # Find the correlated values\n",
    "        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "        drop_features = [column for _ in range(len(corr_features))]    \n",
    "    \n",
    "        # Record the information (need a temp df for now)\n",
    "        temp_df = pd.DataFrame.from_dict({'corr_feature1': drop_features,\n",
    "                                         'corr_feature2': corr_features,\n",
    "                                         'corr_value': corr_values})\n",
    "    \n",
    "        # Add to dataframe\n",
    "        record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "    drops=[]## 下面是删除规则\n",
    "    if record_collinear.shape[0]==0:\n",
    "        return 'nothing!','nothing'\n",
    "    for i,j in zip(record_collinear.corr_feature1.values.tolist(),record_collinear.corr_feature2.values.tolist()):\n",
    "        if X[i].nunique()>X[j].nunique():\n",
    "            drops.append(j)\n",
    "        else:\n",
    "            drops.append(i)\n",
    "    drops=list(set(drops))\n",
    "    return record_collinear,drops\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 超参数优化模板\n",
    "下面列出了最常用的hyperot和bayesaionoptimization，sciki-optimize也很方便，然后就是deap和其对应的sklearn-deap，在实践中，遗传算法没有贝叶斯优化来的好用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "贝叶斯优化之hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import time\n",
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(sample_submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}\n",
    "\n",
    "# Set algoritm parameters\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=27)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "贝叶斯优化之基于高斯过程的 贝叶斯优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              #'learning_rate' : learning_rate,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(train_df))\n",
    "    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx][target].values)\n",
    "    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx][target].values)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n",
    "    \n",
    "    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n",
    "    \n",
    "    score = roc_auc_score(train_df.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (31, 500), \n",
    "    'min_data_in_leaf': (20, 200),\n",
    "    'bagging_fraction' : (0.1, 0.9),\n",
    "    'feature_fraction' : (0.1, 0.9),\n",
    "    #'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'reg_alpha': (1, 2), \n",
    "    'reg_lambda': (1, 2),\n",
    "    'max_depth':(-1,50),\n",
    "}\n",
    "\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init_points = 10\n",
    "n_iter = 15\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "    \n",
    "    \n",
    "LGB_BO.max['target']\n",
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "遗传算法 deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "#######################\n",
    "# Load & process data\n",
    "#######################\n",
    "def get_data():\n",
    "    #################\n",
    "    # read datasets\n",
    "    #################\n",
    "    train = pd.read_csv('../input/train.csv')\n",
    "    test_submit = pd.read_csv('../input/test.csv')\n",
    "\n",
    "    # Get y and ID\n",
    "    train = train[train.y < 250] # Optional: Drop y outliers\n",
    "    y_train = train['y']\n",
    "    train = train.drop('y', 1)\n",
    "    test_submit_id = test_submit['ID']\n",
    "\n",
    "    #########################\n",
    "    # Create data\n",
    "    #########################\n",
    "    features = ['X0',\n",
    "                'X5',\n",
    "                'X118',\n",
    "                'X127',\n",
    "                'X47',\n",
    "                'X315',\n",
    "                'X311',\n",
    "                'X179',\n",
    "                'X314',\n",
    "                'X232',\n",
    "                'X29',\n",
    "                'X263',\n",
    "                'X261']\n",
    "\n",
    "    # Build a new dataset using key parameters, lots of drops\n",
    "    train = train[features]\n",
    "    test_submit = test_submit[features]\n",
    "\n",
    "    # Label encoder\n",
    "    for c in train.columns:\n",
    "        if train[c].dtype == 'object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train[c].values) + list(test_submit[c].values))\n",
    "            train[c] = lbl.transform(list(train[c].values))\n",
    "            test_submit[c] = lbl.transform(list(test_submit[c].values))\n",
    "\n",
    "    # Convert to matrix\n",
    "    train = train.as_matrix()\n",
    "    y_train = np.transpose([y_train.as_matrix()])\n",
    "    test_submit = test_submit.as_matrix()\n",
    "    test_submit_id = test_submit_id.as_matrix()\n",
    "\n",
    "    return train, y_train, test_submit, test_submit_id\n",
    "\n",
    "#########################\n",
    "# XGBoost Model\n",
    "#########################\n",
    "def gradient_boost(train, y_train, params):\n",
    "    y_mean_train = np.mean(y_train)\n",
    "\n",
    "    # prepare dict of params for xgboost to run with\n",
    "    xgb_params = {\n",
    "        'n_trees': params[0],\n",
    "        'eta': params[1],\n",
    "        'max_depth': params[2],\n",
    "        'subsample': params[3],\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'base_score': y_mean_train,\n",
    "        'seed': 123456789,\n",
    "        'silent': 1\n",
    "    }\n",
    "\n",
    "    # form DMatrices for Xgboost training\n",
    "    dtrain = xgb.DMatrix(train, y_train)\n",
    "\n",
    "    # xgboost, cross-validation\n",
    "    cv_result = xgb.cv(xgb_params,\n",
    "                       dtrain,\n",
    "                       nfold = 10,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100,\n",
    "                       verbose_eval=False,\n",
    "                       show_stdv=False\n",
    "                      )\n",
    "\n",
    "    num_boost_rounds = len(cv_result)\n",
    "\n",
    "    # train model\n",
    "    model = xgb.train(dict(xgb_params), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "    # get model accuracy\n",
    "    accuracy = r2_score(dtrain.get_label(), model.predict(dtrain))\n",
    "\n",
    "    return model, accuracy\n",
    "\n",
    "######################\n",
    "# Genetic algorithm\n",
    "######################\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Get data\n",
    "train, y_train, test_submit, test_submit_id = get_data()\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register(\"n_trees\", random.randint, 100, 10000)\n",
    "toolbox.register(\"eta\", random.uniform, 0.0001, 0.01)\n",
    "toolbox.register(\"max_depth\", random.randint, 1, 10)\n",
    "toolbox.register(\"subsample\", random.uniform, 0, 1)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.n_trees, toolbox.eta, toolbox.max_depth, toolbox.subsample), n=1)\n",
    "\n",
    "# define the population to be a list of individuals\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# the goal ('fitness') function to be maximized\n",
    "def evalOneMax(individual):\n",
    "    model, accuracy = gradient_boost(train, y_train, individual)\n",
    "    return [accuracy]\n",
    "\n",
    "# the model for a specified individual\n",
    "def getModel(individual):\n",
    "    model, accuracy = gradient_boost(train, y_train, individual)\n",
    "    return model\n",
    "\n",
    "# register the goal / fitness function\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "\n",
    "# register the crossover operator\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "\n",
    "# register a mutation operator\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "\n",
    "# operator for selecting individuals for breeding the next\n",
    "# generation\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(12345)\n",
    "\n",
    "    # create an initial population\n",
    "    pop = toolbox.population(n=20)\n",
    "\n",
    "    # CXPB  is the probability with which two individuals\n",
    "    #       are crossed\n",
    "    #\n",
    "    # MUTPB is the probability for mutating an individual\n",
    "    CXPB, MUTPB = 0.5, 0.2\n",
    "\n",
    "    print(\"Start of evolution\")\n",
    "\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    print(\"  Evaluated %i individuals\" % len(pop))\n",
    "\n",
    "    # Extracting all the fitnesses of\n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "\n",
    "    # Begin the evolution\n",
    "    while max(fits) < 100 and g < 1000:\n",
    "        # A new generation\n",
    "        g = g + 1\n",
    "        print(\"-- Generation %i --\" % g)\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "\n",
    "            # cross two individuals with probability CXPB\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "\n",
    "                # fitness values of the children\n",
    "                # must be recalculated later\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "\n",
    "            # mutate an individual with probability MUTPB\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        print(\"  Evaluated %i individuals\" % len(invalid_ind))\n",
    "\n",
    "        # The population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x * x for x in fits)\n",
    "        std = abs(sum2 / length - mean ** 2) ** 0.5\n",
    "\n",
    "        print(\"  Min %s\" % min(fits))\n",
    "        print(\"  Max %s\" % max(fits))\n",
    "        print(\"  Avg %s\" % mean)\n",
    "        print(\"  Std %s\" % std)\n",
    "\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        print(\"Best individual so far is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
    "\n",
    "    print(\"-- End of (successful) evolution --\")\n",
    "\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速固定随机性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 最常见的特征衍生通用方案，结合feature importance来衍生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(nm,', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=X_train,test=X_test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(nm,', ',end='')\n",
    "        \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=X_train,df2=X_test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 目前kaggle上predict 清一色的套路方法，不再是单模而是交叉验证形成多个模型然后集成输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n",
    "    start_time = time()\n",
    "    print('Training on fold {}'.format(fold + 1))\n",
    "    \n",
    "    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n",
    "    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n",
    "    aucs.append(clf.best_score['valid_1']['auc'])\n",
    "    \n",
    "    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 两个经典工具模板，给了一个大框架写了一些具体的实例，可以根据自己的需求自己设计一些用的顺手的工具或者把流程性的代码保存下来等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/Ynakatsuka/kaggle_utils   工具模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection   流程性代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  特征偏移的检验与缓解办法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "（1）、检验方法有对抗性验证、kris的验证法，我之前尝试过用psi这类过滤式的指标来衡量特征的偏移程度，但是整体上没有kris的验证方法来的直观，因为这种验证方法直接将特征的偏移程度通过我们关心的评价指标反应出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "（2）、一些常用的缓解方法\n",
    "连续特征：\n",
    "1、对数变换等，对数变换属于box-cox变换的一种，sklearn中的Transform功能或者scipy的box-cox功能可实现这类变换；\n",
    "2、分箱，有效改善特征偏移问题，但是需要作为超参数调整比如分箱数量，分箱算法选择等，比较麻烦，可以考虑结合kris验证方法，生成新的分箱特征之后测试一下泛化误差的大小；\n",
    "3、加减乘除，例如时间相关特征可以考虑与时间特征做差或者做除来消除时间相关性；\n",
    "4、待补充\n",
    "\n",
    "类别特征：\n",
    "1、少数类别合并；\n",
    "2、类别转int类型特征；\n",
    "3、类别特征编码并删除原特征或者原特征转连续特征；\n",
    "4、待补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu",
   "language": "python",
   "name": "ml_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
